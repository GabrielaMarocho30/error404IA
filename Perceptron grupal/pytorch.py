# -*- coding: utf-8 -*-
"""PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iMdEWOEm2ug8rBvAJHdLh3Bi2UCRgmGM

# **PyTorch**
"""

import torch
import torch.nn as nn
import torch.optim as optim

# Paso 1: Inicializar los pesos W y el umbral θ en valores pequeños aleatorios
# Esto lo realiza automáticamente PyTorch al crear la capa lineal en la clase Perceptrón.
class Perceptron(nn.Module):
    def __init__(self):
        super(Perceptron, self).__init__()
        self.fc = nn.Linear(2, 1)  # 2 entradas, 1 salida (peso y umbral se inicializan aquí automáticamente)

    # Paso 3: Propagar - Calcular Y en función de X
    # La función forward realiza XW - θ y aplica la función de activación f (sigmoide en este caso)
    def forward(self, x):
        return torch.sigmoid(self.fc(x))  # Aplicar la función de activación sigmoide

# Paso 2: Leer un par de entrenamiento X = {x1, x2, ..., xn} y las salidas deseadas D = {d1}
X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)  # Entradas
D = torch.tensor([[0], [0], [0], [1]], dtype=torch.float32)  # Salidas deseadas (puerta lógica AND)

# Crear el modelo del perceptrón
model = Perceptron()

# Paso 4: Calcular el error (Error = D - Y)
# Usamos la función de pérdida Binary Cross-Entropy Loss para medir la diferencia entre D y Y
criterion = nn.BCELoss()  # Pérdida binaria para clasificación

# Paso 5: Retropropagar - Calcular W_t en función de Y y D
# Utilizamos el optimizador SGD (Stochastic Gradient Descent) con una tasa de aprendizaje de 0.1
optimizer = optim.SGD(model.parameters(), lr=0.1)

# Entrenamiento del modelo (Paso 6: Repetir hasta obtener un error aceptable o completar las iteraciones)
epochs = 100  # Número de iteraciones o épocas
for epoch in range(epochs):
    optimizer.zero_grad()  # Reiniciar los gradientes acumulados
    y_pred = model(X)  # Propagación hacia adelante: calcular Y en función de X
    loss = criterion(y_pred, D)  # Calcular el error (función de pérdida)
    loss.backward()  # Retropropagación: calcular los gradientes de los pesos y el umbral
    optimizer.step()  # Actualizar los pesos y el umbral usando los gradientes calculados

    # Mostrar el error en cada 10 épocas
    if (epoch + 1) % 10 == 0:
        print(f"Época {epoch + 1}/{epochs}, Pérdida: {loss.item():.4f}")

# Evaluación final del modelo
print("\nPesos finales y sesgo:")
print(f"Pesos: {model.fc.weight.data}")  # Pesos aprendidos después del entrenamiento
print(f"Sesgo: {model.fc.bias.data}")  # Umbral aprendido después del entrenamiento

# Predicciones
print("\nPredicciones:")
with torch.no_grad():  # Desactivar cálculo de gradiente durante la evaluación
    predicciones = model(X)  # Propagación hacia adelante para obtener predicciones
    for i, pred in enumerate(predicciones):
        # Mostrar cada entrada, la predicción generada por el modelo y el valor esperado
        print(f"Entrada: {X[i].numpy()}, Predicción: {pred.item():.4f}, Esperado: {D[i].item()}")

import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense

# Datos de entrada (combinaciones binarias para una puerta AND)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
D = np.array([0, 0, 0, 1], dtype=np.float32)

# Construcción del modelo usando Keras
modelo = Sequential()
modelo.add(Dense(1, input_dim=2, activation='sigmoid'))  # Capa con 1 neurona y activación sigmoide

# Compilación del modelo
modelo.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])

# Entrenamiento del modelo
modelo.fit(X, D, epochs=500, verbose=1)

# Predicciones
predicciones = modelo.predict(X)
print("Predicciones del modelo (redondeadas):")
print(np.round(predicciones))  # Redondeamos las predicciones a 0 o 1